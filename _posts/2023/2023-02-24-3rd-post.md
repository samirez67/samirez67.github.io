---
layout: post
title: Assignment 2
excerpt: "Textual Analysis Assignment"
modified: 4/29/2023, 9:00:24
tags: [intro, beginner, corpus, tutorial]
comments: true
category: blog
---

# **A distant analysis of a corpus of popular texts using intruments of Voyant tools**

A distant analysis of a corpus of some of the most popular thriller novels to find out what and how does distant textual analysis tell us about a corpus of texts that a reguar close reading might not have.

 - ## Preface 

For my analysis I decided to go for the corpus of some of the most popular works of renowned British novelist and the author of the Sherlock Holme franchise. Sir Arthur Conan Doyle. 

### About the Author:
Even aside from the quality of his literary pieces and the acclaim he has garnered for it throughout time, Sir Arthur's personal life is very interesting in itself. He is the sort of person that has had both the fortune and fortitude to have done seemingly everything throughout the span of his life to various degrees: starting from being a acclaimed doctor and physicist all the way upto having a fairly successful political career and even having experience in the sporting arena in the form of playing cricket for a professional county team in his hometown. This in itself certainly does enough to paint a picture that shows the simply staggering sloth of knowledge a man like him possesses, something that becomes all the more evident in the sheer quantity and the diversity of his works, aside from the critically-acclaimed quality itself. Other than Holmes stories, his works included fantasy and science fiction stories about Professor Challenger, and humorous stories about the Napoleonic soldier Brigadier Gerard, as well as plays, romances, poetry, non-fiction, and historical novels. One of Doyle's early short stories, "J. Habakuk Jephson's Statement" (1884), helped to popularise the mystery of the Mary Celeste.





### The works:

For our analysis, we created a corpus consisting of excerpts from the following texts:

1. A study in Scarlet.
2. The Adventures of Sherlock Holmes.
3. The Return of Sherlock Holmes
4. The Hounds of Baskervilles.

The tool used for the textual analysis of the corpus was Voyant tools as that gives us a rather user-friendly set of tools to gather and breakdown the corpus into pieces, aside from having a wide array of visualisation and analysing tools itself.







 

> ### Why though? What does computer-assisted analysis allow us to see that we wouldn't have seen in a linear read?


Exploratory data analysis (EDA) is a statistical approach used to analyze and summarize datasets. In the context of textual corpora, EDA can help us understand the characteristics of the data, identify patterns and trends, and generate hypotheses for further analysis. Some common techniques used in EDA of textual data include:

Such textual analysis using a platform like Voyant tools help us understand a text, or in this case, a corpus, in more than one way. The speciality and significance of such analysis is nicely put by Ama Bemma Adwetewa-Badu in her guest appearance in a [talk](https://newbooksnetwork.com/distant-reading) in which she refers to the term ‘distant reading’.  She does that by pulling in a contrast to its opposite and the more well-known counterpart close reading which can be defined as the sort of careful, slow, fine grained examination of a text or a particular set of text. It can also be characterised by the ‘close and attentive reading’ of a text to etch out the minute details of the text - something that might be common in a college classroom.

Distant reading on the other hand enables the development of critical insights by aggregating a large body of text together as opposed to relying on close reading from a restricted canon of text. This process, as is the intent behind it, enables us to do a large scale examination of literary history tree and for a much broader contextual analysis of the literature.


The analytical advantages and the implications of such computational techniques in the field of textual analysis is immense and innumerable. Some of the most useful and significant of them include, but are not limited to: 


1. Text Preprocessing: This involves cleaning and transforming raw text data into a more usable format. Text preprocessing techniques include tokenization, stopword removal, stemming, and lemmatization.

2. Text Mining: This refers to the process of extracting useful information and patterns from textual data. Techniques used in text mining include topic modeling, sentiment analysis, and named entity recognition.

3. Natural Language Processing (NLP): This involves using algorithms and statistical models to analyze and understand human language. NLP techniques include part-of-speech tagging, dependency parsing, and machine translation.

4. Word frequency analysis: This involves identifying the most frequently occurring words in a corpus, which can provide insight into the main topics and themes.

5. Word clouds: A visual representation of word frequency in which the size of each word corresponds to its frequency.

6. Topic modelling: This technique involves identifying the underlying topics and themes in a corpus using statistical methods.

7. Sentiment analysis: This involves analyzing the sentiment or emotion expressed in a text, which can provide insight into the attitudes and opinions of the author.


Overall, exploratory data analysis can help us better understand textual data by providing a comprehensive overview of its characteristics and identifying patterns and trends that may be of interest for further analysis.


 - ## The results:

> *What are the analytics we get from the corpus and what do we understand from it (that we wouldn’t have from plain reading it)*

<figure>
<img src="/assets/wordcloud.png" style="width:110%; height:110%;"/>
<figcaption style="text-align: center;">Wordcloud comprising of the most commonly occuring 155 words throughout the corpus</figcaption>
</figure>


Some of the most frequent words in the corpus (not considering filler words like prepositions and common verbs):

 - Holmes: Which makes sense, being the name of the main protagonist of the entire series/franchise.
 - Watson: The name of the Mr. Holmes’s assistant, the side character and the secondary protagonist of the plot
 - Case: which makes sense, SHerlock is a renowned detective of downtown London who is constantly faced with critical ‘cases’ to solve
 - Night: which sort of explains the nature of his cases and gives us a feel of the nature of the plot itself
 - Face: an interesting feature; maybe highlighting the impeccable skills of Holmes when it comes to ‘reading’ people.
 - These are generally the sort of stuff that we would normally come to expect from something like the Sherlock Holmes franchise: a crime-detective-thriller series. Nothing surprising there

We could go further and analyse the most frequent words throughout each of the individual books included in out corpus

<figure>
<img src="/assets/summary.png" style="width:250%; height:250%;"/>
<figcaption style="text-align: center;">Summary of word counts</figcaption>
</figure>


 - ### A study in Scarlett: 

Written way back in 1887, A Study in Scarlet was the first novel in which Sherlock Holmes appeared. It is one of the only four novels in which Sir Arthur Conan Doyle portrayed the iconic detective, other than the short stories. A Study in Scarlet is also the first English language detective fiction to incorporate the magnifying glass as a tool for investigation.

A Study in Scarlet is The quintessential Conan Doyle mystery novel; the first of its kind, and also the one that sets his reputation for his signature narrative style that is carried through all the other novels and stories (the Holmes series and the others) that followed. It is set in downtown London (where Holmes is primarily from) while the latter half of the story takes a dip into the past for a flashback (of the antagonist) that is set in the American West.

The first part of the story is quite fast-paced. Dr. John Watson, the first narrator of the story meets the young, quirky, and freakish genius Sherlock Holmes, and they become flatmates. The story establishes the young Holmes as the protagonist as soon as the two future friends meet. And thereon, Holmes never fails to blow Watson’s mind. Honestly I personally did not enjoy the first part of the story all that much as everything seemed a bit too exaggerated. However, the end of the chapter was quite a surprise. 
The same cannot be said going forward into the latter half of the story however.The second part was absolutely phenomenal as the storytelling prowess of an author as revered as Conan Doyle was on display. Detailing the story brings a life-like depiction. And more than anything, the part that struck me the most was how the author narrates a long and complex story with few words, without losing the essence even for a moment. You believe what he says, as he completely pulls you in with his captivating narration.
That being said, I was only able to properly understand and contextualise the contents of the output we got from Voyant tools only after reading the entire text itself (or at least the abridged version of it). The most frequent words are, for most part, the names of the most significant characters (spoiler alert) of the story itself. And even the ranks amongst them makes a lot more sense only once someone has read or knows the entire plot of the novel. And hence I shall also not say anymore regarding this part as to not provide any further spoilers and do justice to the exceptionally amazing read that this novel is.
Some of the other notable occurrences in the word lists included Utah (9, where the chronologically earlier part of the plot was set) and pills (10, a key component of the entire mystery. Sorry for the spoiler again, this’ll be the last I promise).


This, unfortunately, is also the pattern that flows through the analytics we got for all the other texts of the corpus: there are quite a few chunks and insights to be had, but they are technically incomprehensible at very best and totally meaningless at worst as someone who might not have any idea of the stories or the plot whatsoever would not possibly be able to make any sense of the information acquired. 

<figure>
<img src="/assets/frequency chart.png" style="width:250%; height:250%;"/>
<figcaption style="text-align: center;">Frequency Chart of the top 5 most common words</figcaption>
</figure>


Of the most frequent words detected, most of them being nondescript filler words, the word Holmes sees a late surge towards the end, indicating a spike in frequencies of Sherlock’s direct involvements towards the end of the stories. This reflects the climatic ending of each of the stories and their plots, a feature very common in the narration of the works of Conan Doyle.

The rest of the words maintain a steady frequency throughout, providing nothing of much interest. Aside from the word sir, which I’ll assume is an indicator of the same trend mentioned previously, albeit this being Watson speaking as Sherlock is obviously never going to address those mere normal humans as ‘sir’.

 - ## Findings


I wouldn't say I had any ‘surprises’ throughout the entire analysis but I did come across a few interesting findings throughout: 

Unlike contemporary thriller novels, the ones of Conan Doyle have quite a steady tempo from the gte go before amping it up towards the latter half of the text, while delaying the climax as late as possible that gives the novels the original Conal Doyle feel that have been revered and followed as a staple in the genre of mystery thrillers ever since.
The novels are very character-focused: the plot development relies a great deal on the narration and the direct involvement of the characters involved, giving the novels a life-like feel to it.
The narration style of Conan Doyle is sharp and potent; using fewer words to express bigger contexts, pretty comprehensively on that, not diluting the tempo of the story at all that would’ve happened for the use of unnecessary filler words.
The impeccable knowledge of Conan Doyle is on full display with his immaculate portrayal of everything one could imagine starting from an accurate description of downtown London and its social hierarchy to his extensive familiarity with human anatomy. 




 - ## Language Compatibility
In terms of language compatibility I’m not sure if we can expect the same degree of efficiency and or quality in terms of output, if any. That is because for certain languages like Chinese or Japanese, the syntax of words are not similar to that of English; they have a character based system of lexicons where each character (or alphabet) represents a word. Even aside from such sorts of languages which do have a similar syntax to that of English might not have the same sort of syllables or word formation which might not aid the tokenization of words that is done to distinguish each unique word from another on a platform like Voyant. That is unless there is a language-oriented version of such an analysing software designated for that particular language.

In short, given the language limitations of Voyant and the array of complexities provided by each and every one of the different languages themselves, we cannot expect the same degree of success in terms of raw outputs to further work with from the texts of that respective language.


- ## Limitations

1. Scarcity of insights or information that might be of substantial use to any reader unless that person has already had knowledge of the text/plot itself.
2. An abundance of filler words that dilutes the results.
3. Other analytics tools that might be of greater significance and give us much more useful outputs to work with (ones like Cirrus, StreamGraph, TextualArcs, to name a few) but demand a certain level of technical aptitude to interpret/understand.









 - ## Final thoughts:


As said by Ama Bemma Adwetewa-Badu in the podcast, such sort of distant reading through a textual analysis of a bigger corpus of texts open the door to newer possibilities and understandings that might ot have been possible through a minute, close readings of the text. This holds true for this corpus of texts as much as it would’ve for any other. However, the output and the validity and the utility of it, as we learned now, also depends a great lot on the nature of the content itself.

In our case, the utility and the readability of the analytics were hindered by a lack of unique terms and any subsequent patterns deriving from them. Had we worked with a corpus of texts with a more historical essence like non-fictions, the true analytical significance of these tools and techniques would have come to the fore as they would’;ve enabled us to derive a much broader patterns throughout these texts over the course of time; something that we might not have been able to derive otherwise. It was a good experiment however, that leaves with a lot of food for thought regarding the potential of both the tools and the theories behind it.

For now however, I cannot stress enough why you **must** read the novels that I picked for these experiments. Reading them made the whole thought-process and the time invested in it worth it if nothing else.








